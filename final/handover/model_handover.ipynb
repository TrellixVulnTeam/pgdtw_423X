{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the various elements of the predictive model built. The corresponding file is `model.py`\n",
    "\n",
    "The module contains two functions:\n",
    "- `build_structured_array`\n",
    "- `generate_dataset_xy`\n",
    "\n",
    "and one class:\n",
    "- `Estimator`\n",
    "\n",
    "The two functions make the data available to the estimator object.\n",
    "\n",
    "---\n",
    "`build_structured_array(data_set)` shapes the output variable, the survival time of a batch, as a structured array, that is the data structure accepted by the model in the `sksurv` package that is used in this estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structured_array(data_set):\n",
    "    output = list()\n",
    "    for idx, row in data_set.iterrows():\n",
    "        survival_time = row['true_length'] - row['length']\n",
    "        output.append((True, survival_time))\n",
    "    res = np.array(output, dtype = [('status', bool), ('time_remaining', 'f8')])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`generate_dataset_xy(t_ref, t, ongoing_id, D, data)` creates a dataset (predictors and output variable) based on `t_ref`, the mapped point on the reference, `t`, the present length of the ongoing query, `D`, the `Dtw` object, and `data`, containing the information about all the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_xy(t_ref, t, ongoing_id, D, data):\n",
    "    data_set = list()\n",
    "\n",
    "    for _id, warp_dist in D.data_open_ended['warp_dist'].items():\n",
    "        if _id == ongoing_id:\n",
    "            mapped_points = list(filter(lambda x: (x[0] == t_ref and x[1] == t), warp_dist))\n",
    "        else:\n",
    "            mapped_points = list(filter(lambda x: x[0] == t_ref, warp_dist))\n",
    "        for (i, j, d) in mapped_points:\n",
    "            data_point = {'DTW_distance': d,\n",
    "                          'length': j + 1,\n",
    "                          'query_id': _id,\n",
    "                          'true_length': len(data[_id][0]['values'])}\n",
    "            data_set.append(data_point)\n",
    "\n",
    "    data_set = pd.DataFrame(data_set)\n",
    "    data_set.index = data_set['query_id']\n",
    "\n",
    "    data_y = build_structured_array(data_set)\n",
    "    data_set.drop(columns=['query_id', 'true_length'], inplace=True)\n",
    "\n",
    "    for _id, row in data_set.iterrows():\n",
    "        batch = D.data['queries'][_id]\n",
    "        length = int(row['length'])\n",
    "        for pv in batch:\n",
    "            data_set.at[_id, pv['name']] = pv['values'][length - 1]\n",
    "\n",
    "    return (data_set, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here we present the content of the `Estiamtor` class. It contatins the following 6 methods:\n",
    "\n",
    "- `__init__`\n",
    "- `fit`\n",
    "- `predict`\n",
    "- `score`\n",
    "- `get_params`\n",
    "- `set_params`\n",
    "\n",
    "`__init__(dtw_obj, regressor=LinearRegression(), loss='coxph', learning_rate=0.1, n_estimators=100, max_depth=3, subsample=1.0, random_state=42)` takes as arguments the `Dtw` object used for the alignment, the `regressor` to use on the estimated risk to obtain time estimates, and  `loss`, `learning_rate`, `n_estiamtors`, `max_depth`, `subsample` that are the parameters of the `GradientBoostingSurvivalAnalysis` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, dtw_obj, regressor=LinearRegression(), loss='coxph', learning_rate=0.1, n_estimators=100, max_depth=3, subsample=1.0, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        self.regressor = regressor\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.dtw_obj = dtw_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`fit(x_train, y_train)` This method fits the `GradientBoostingSurvivalAnalysis` model to the data and predicts the risk on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, x_train, y_train):\n",
    "        self.model = GradientBoostingSurvivalAnalysis(loss=self.loss,\n",
    "                                                      learning_rate=self.learning_rate,\n",
    "                                                      n_estimators=self.n_estimators,\n",
    "                                                      max_depth=self.max_depth,\n",
    "                                                      subsample=self.subsample,\n",
    "                                                      random_state=self.random_state)\n",
    "\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.model.fit(self.x_train, self.y_train)\n",
    "\n",
    "        self.data_set = pd.concat([self.x_train, pd.Series(\n",
    "            data=self.y_train['time_remaining'], index=self.x_train.index, name='time_remaining')], axis=1, sort=False)\n",
    "        self.data_set['risk'] = self.model.predict(self.x_train)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`predict(new_x, by='risk')` returns a time estimate on the new point `x_new`. The prediciton happens in two steps. First, the survival analysis algorithm estimate the risk, than the regression model estimates an actual time to completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, new_x, by='risk'):\n",
    "        x_new = pd.DataFrame(deepcopy(new_x))\n",
    "        x_new['risk'] = self.model.predict(x_new)\n",
    "        query_id = list(x_new.index)[0]\n",
    "        x_length = len(self.dtw_obj.data['queries'][query_id][0]['values'])\n",
    "        x_new['time_remaining'] = x_length - x_new['length']\n",
    "\n",
    "        self.data_set_extd = pd.concat([self.data_set, x_new], axis=0, sort=False)\n",
    "        self.data_set_extd.sort_values(by='risk', ascending=False, inplace=True)\n",
    "\n",
    "        locations = self.data_set_extd.index.get_loc(query_id)\n",
    "\n",
    "        locs = list()\n",
    "        if type(locations) == slice:\n",
    "            start, stop = locations.start, locations.stop\n",
    "            locs.extend([loc for loc in np.arange(start, stop)])\n",
    "        elif type(locations) == int or type(locations) == np.int64:\n",
    "            locs = [locations]\n",
    "        elif type(locations) == np.ndarray:\n",
    "            locs = np.arange(len(locations))[locations]\n",
    "        else:\n",
    "            print('ERROR')\n",
    "            print(type(locations))\n",
    "            locs = []\n",
    "\n",
    "        y_values = self.data_set_extd['time_remaining']\n",
    "\n",
    "        if by == 'rank':\n",
    "            x_values = pd.Series(np.arange(y_values))\n",
    "        elif by == 'risk':\n",
    "            x_values = self.data_set_extd['risk']\n",
    "        elif by == 'scaled_risk':\n",
    "            scaler = MinMaxScaler()\n",
    "            x_values = scaler.fit_transform(self.data_set_extd['risk'])\n",
    "        ests = list()\n",
    "\n",
    "        for loc in locs:\n",
    "            #            print(locs)\n",
    "            #            print([x for x, y in zip(np.arange(len(t_left)), t_left.values)])\n",
    "            xy = [(x, y) for (x, y) in zip(x_values.values, y_values.values) if x != loc]\n",
    "            x = np.array([x[0] for x in xy]).reshape(-1, 1)\n",
    "            y = np.array([x[1] for x in xy])\n",
    "        # Add possibility for risk as X variable\n",
    "            reg = self.regressor.fit(X=x, y=np.log1p(y))\n",
    "            if by == 'scaled_risk':\n",
    "                ests.append(np.expm1(reg.predict(scaler.transform(x_values.values[loc]))[0]))\n",
    "            else:\n",
    "                ests.append(np.expm1(reg.predict(x_values.values[loc])[0]))\n",
    "\n",
    "        return np.array(ests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`score(x_test, y_test)` computes the mean absolute error of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        return np.mean(np.abs(y_pred - y_test['time_remaining']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_params` and `set_params` follow the `sklearn` library in terms of parameter setting (useful during grid search cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(self, deep=True):\n",
    "        return {'dtw_obj': self.dtw_obj,\n",
    "                'regressor': self.regressor,\n",
    "                'loss': self.loss,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'max_depth': self.max_depth,\n",
    "                'subsample': self.subsample}\n",
    "\n",
    "def set_params(self, parameters):\n",
    "    for parameter, value in parameters.items():\n",
    "        setattr(self, parameter, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

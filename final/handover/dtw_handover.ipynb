{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the various elements of the DTW component of the predictive system. The actual module is stored in the `libdtw.py` file.\n",
    "\n",
    "## Preliminaries: Data Loading\n",
    "The `libdtw.py` module contains 2 functions:\n",
    "- `load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\")`\n",
    "- `assign_ref(data)`\n",
    "\n",
    "and one class:\n",
    "- `Dtw(json_obj = False)`\n",
    "\n",
    "We first illustrate the usage of the two funcitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\")` load the .pickle in the `data_path` position. This has to be a dictionary where the keys are the batch IDs and the values are lists of dictionaries representing the PVs (with keys `name, start, end, values`). \n",
    "\n",
    "The function first identifies the median (with respect to the duration parameter) batch to be used as reference. Then, it selects the first `n_to_keep` batches closer to the reference one in terms of duration. The output dictionary has the `reference` key explicitly declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\"):\n",
    "    \"\"\"\n",
    "    Load data of operation 3.26, only the n_to_keep batches with duration closer to the median one\n",
    "    are selected\n",
    "    \"\"\"\n",
    "    with open(data_path, \"rb\") as infile:\n",
    "        data = pickle.load(infile)\n",
    "\n",
    "    operation_length = list()\n",
    "    pv_dataset = list()\n",
    "    for _id, pvs in data.items():\n",
    "        operation_length.append((len(pvs[0]['values']), _id))\n",
    "        pv_list = list()\n",
    "        for pv_dict in pvs:\n",
    "            pv_list.append(pv_dict['name'])\n",
    "        pv_dataset.append(pv_list)\n",
    "\n",
    "    median_len = np.median([l for l, _id in operation_length])\n",
    "\n",
    "    # Select the batches closer to the median bacth\n",
    "    # center around the median\n",
    "    centered = [(abs(l-median_len), _id) for l, _id in operation_length]\n",
    "    selected = sorted(centered)[:n_to_keep]\n",
    "\n",
    "    med_id = selected[0][1]\n",
    "\n",
    "    all_ids = list(data.keys())\n",
    "    for _id in all_ids:\n",
    "        if _id not in [x[1] for x in selected]:\n",
    "            _ = data.pop(_id)\n",
    "\n",
    "    data['reference'] = med_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assign_ref(data)` computes the median batch (acording to the duration parameter) and sets it as `reference` of the data set. It is useful in case the data loaded with `load_data()` undergoes modification prior to its actual use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ref(data):\n",
    "    data = copy(data)\n",
    "    operation_length = list()\n",
    "    pv_dataset = list()\n",
    "    for _id, pvs in data.items():\n",
    "        operation_length.append((len(pvs[0]['values']), _id))\n",
    "        pv_list = list()\n",
    "        for pv_dict in pvs:\n",
    "            pv_list.append(pv_dict['name'])\n",
    "        pv_dataset.append(pv_list)\n",
    "\n",
    "    median_len = np.median([l for l, _id in operation_length])\n",
    "\n",
    "    # Select the ref_len=50 closest to the median bacthes\n",
    "    # center around the median\n",
    "    centered = [(abs(l-median_len), _id) for l, _id in operation_length]\n",
    "    selected = sorted(centered)\n",
    "\n",
    "    med_id = selected[0][1]  # 5153\n",
    "\n",
    "    all_ids = list(data.keys())\n",
    "    for _id in all_ids:\n",
    "        if _id not in [x[1] for x in selected]:\n",
    "            _ = data.pop(_id)\n",
    "\n",
    "    data['reference'] = med_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dtw class\n",
    "The `Dtw(json_obj)` contains all the methods used to set up the DTW algorithm, optimizing the variables weights, computing the alignment. it contains the following methods (divided by topic here for clarity):\n",
    "\n",
    "##### data handling\n",
    "- `__init__()`\n",
    "- `convert_data_from_json()`\n",
    "- `add_query`\n",
    "- `get_scaling_parameters`\n",
    "- `remove_const_feats`\n",
    "- `scale_pv`\n",
    "- `convert_to_mvts`\n",
    "\n",
    "##### dtw implementation\n",
    "- `comp_dist_matrix`\n",
    "- `comp_acc_dist_matrix`\n",
    "- `comp_acc_element`\n",
    "- `get_warping_path`\n",
    "- `call_dtw`\n",
    "- `dtw`\n",
    "- `get_ref_prefix_length`\n",
    "- `itakura`\n",
    "- `extreme_itakura`\n",
    "- `check_open_ended`\n",
    "\n",
    "##### step pattern selection utilities\n",
    "- `time_distortion`\n",
    "- `avg_time_distortion`\n",
    "- `avg_distance`\n",
    "- `get_p_max`\n",
    "- `get_global_p_max`\n",
    "\n",
    "##### variables weights optimization\n",
    "- `reset_weights`\n",
    "- `compute_mld`\n",
    "- `extract_single_feat\n",
    "- `weight_optimization_single_batch`\n",
    "- `weight_optimization_step`\n",
    "- `optimize_weights`\n",
    "- `get_weight_variables`\n",
    "\n",
    "##### visualization\n",
    "- `distance_cost_plot`\n",
    "- `plot_weights`\n",
    "- `plot_by_name`\n",
    "- `do_warp`\n",
    "- `plot_warped_curves`\n",
    "\n",
    "##### misc\n",
    "- `online_scale`\n",
    "- `online_query`\n",
    "- `generate_train_set`\n",
    "\n",
    "We now examine each method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling\n",
    "`__init__(json_obj=False, random_weights = True, scaling='group')` initialize the `Dtw` class. \n",
    "\n",
    "- `json_obj` is the dictionary containing the data in the output format of `load_data()`. \n",
    "- `random_weights` if True, initialize the variable weights to randomly chosen values in the [0.1, 1] interval. \n",
    "- `scaling` is the scaling strategy for the PVs: `group` scales the PVs according to the values of the PVs in the reference batch, `single` scales the PVs as individual entities\n",
    "\n",
    "The initialization consists in structuring the data inside the Dtw object, removing the constant features (in the reference batch) filtering the batches that does not contain all the PVs of the reference batch, and finally setting the variables weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, json_obj=False, random_weights = True, scaling='group'):\n",
    "    \"\"\"\n",
    "    Initialization of the class.\n",
    "    json_obj: contains the data in the usual format\n",
    "    \"\"\"\n",
    "    if not json_obj:\n",
    "        pass\n",
    "    else:\n",
    "        self.convert_data_from_json(deepcopy(json_obj))\n",
    "        #self.scale_params = self.get_scaling_parameters()\n",
    "        self.remove_const_feats()\n",
    "        self.reset_weights(random=random_weights)\n",
    "        self.scaling = scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`convert_data_from_json(json_obj)` takes as input the data object specified before and separates it into reference and query batches. It also initialize the internal structure of the `Dtw` object. This includes the structure to collect data useful in subsequent operations and avoid repeating calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_from_json(self, json_obj):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing all the data, organized as:\n",
    "        ref_id: the ID of the reference batch\n",
    "        reference: reference batch in the usual format (list of dictionaries)\n",
    "        queries: list of dictionaries in which the keys are the query batch's ID and the values are\n",
    "        the actual batches (list of dictionaries)\n",
    "        num_queries: number of query batches in the data set\n",
    "        \"\"\"\n",
    "        ref_id = json_obj[\"reference\"]\n",
    "        reference = json_obj[ref_id]\n",
    "        queries = {key: batch for key, batch in json_obj.items() if key !=\n",
    "                   \"reference\" and key != ref_id}\n",
    "\n",
    "        self.data = {\"ref_id\": ref_id,\n",
    "                     \"reference\": reference,\n",
    "                     \"queries\": queries,\n",
    "                     \"num_queries\": len(queries),\n",
    "                     \"warpings\": dict(),\n",
    "                     \"distances\": dict(),\n",
    "                     'warp_dist': dict(),\n",
    "                     \"queriesID\": list(queries.keys()),\n",
    "                     \"time_distortion\": defaultdict(dict),\n",
    "                     \"distance_distortion\": defaultdict(dict),\n",
    "                     'warpings_per_step_pattern': defaultdict(dict),\n",
    "                     'feat_weights': 1.0}\n",
    "\n",
    "        self.data_open_ended = {\"ref_id\": ref_id,\n",
    "                                \"reference\": reference,\n",
    "                                \"queries\": defaultdict(list),\n",
    "                                'warp_dist': dict()}\n",
    "        scale_params = dict()\n",
    "\n",
    "        for pv_dict in self.data['reference']:\n",
    "            pv_name = pv_dict['name']\n",
    "            pv_min = min(pv_dict['values'])\n",
    "            pv_max = max(pv_dict['values'])\n",
    "            scale_params[pv_name] = (pv_min, pv_max)\n",
    "\n",
    "        self.scale_params = scale_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`add_query(batch_dict)` adds a batch (in the forms of a dict(batch_id: list_of_PVs)) to the `Dtw` object, updating the relevant structures and filtering the PVs. If the batch does not contain all the necessary PVs, it is not added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_query(self, batch_dict):\n",
    "    _id, pvs = list(batch_dict.items())[0]\n",
    "    self.data['queries'][_id] = pvs\n",
    "    self.data['num_queries'] += 1\n",
    "    self.data['queriesID'].append(_id)\n",
    "    self.remove_const_feats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_scaling_parameters()` probably can be omitted, TO CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling_parameters(self):\n",
    "        \"\"\"\n",
    "        Computes the parameters necessary for scaling the features as a 'group'.\n",
    "        This means considering the mean range of a variable across al the data set.\n",
    "        This seems creating problems, since the distributions for the minimum and the\n",
    "        maximum are too spread out. This method is here just in case of future use and to help\n",
    "        removing non-informative (constant) features.\n",
    "        avg_range = [avg_min, avg_max]\n",
    "        \"\"\"\n",
    "        scale_params = dict()\n",
    "\n",
    "        for pv_dict in self.data['reference']:\n",
    "            pv_name = pv_dict['name']\n",
    "            pv_min = min(pv_dict['values'])\n",
    "            pv_max = max(pv_dict['values'])\n",
    "\n",
    "            scale_params[pv_name] = [[pv_min], [pv_max]]\n",
    "\n",
    "        for _id, batch in self.data['queries'].items():\n",
    "            for pv_dict in batch:\n",
    "                pv_name = pv_dict['name']\n",
    "                pv_min = min(pv_dict['values'])\n",
    "                pv_max = max(pv_dict['values'])\n",
    "\n",
    "                scale_params[pv_name][0].append(pv_min)\n",
    "                scale_params[pv_name][1].append(pv_max)\n",
    "\n",
    "        pv_names = scale_params.keys()\n",
    "        for pv_name in pv_names:\n",
    "            scale_params[pv_name] = np.median(scale_params[pv_name], axis=1)\n",
    "\n",
    "        return scale_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`remove_const_feats()` removes the features that are constant in the reference batch from all the batches, reference and queries. If a query batch does not contain all the PVs of the filtered reference one, it is removed from the `Dtw` object. \n",
    "\n",
    "If a Pv is constant in the reference batch, this does not mean that it is constant also in any of the queries, but since all the DTW alignment are performed with respect to one single reference, this PVs would add no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_const_feats(self):\n",
    "        \"\"\"\n",
    "        Removes non-informative features (features with low variability)\n",
    "        \"\"\"\n",
    "        const_feats = list()\n",
    "        for pv_name, avg_range in self.scale_params.items():\n",
    "            if abs(avg_range[0]-avg_range[1]) < 1e-6:\n",
    "                const_feats.append(pv_name)\n",
    "        #const_feats.append('ba_TCzWpXo')\n",
    "        #const_feats.append('ba_TCfg3Yxn')\n",
    "        #const_feats.append('ba_FQYXdr6Q0')\n",
    "\n",
    "\n",
    "        initial_queries = list(self.data['queries'].keys())\n",
    "        print('Number of queries before filtering: %d'%len(initial_queries))\n",
    "\n",
    "        self.data['reference'] = list(filter(lambda x: x['name'] not in const_feats, self.data['reference']))\n",
    "        pv_names = [pv['name'] for pv in self.data['reference']]\n",
    "        for _id in initial_queries:\n",
    "            self.data['queries'][_id] = list(filter(lambda x: x['name']  in pv_names, self.data['queries'][_id]))\n",
    "            if len(self.data['queries'][_id]) != len(self.data['reference']):\n",
    "                _ = self.data['queries'].pop(_id)\n",
    "        print('Number of queries after filtering: %d'%len(self.data['queries']))\n",
    "\n",
    "        self.data['num_queries'] = len(self.data['queries'])\n",
    "        self.data['queriesID'] = list(self.data['queries'].keys())\n",
    "        self.pv_names = pv_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`scale_pv(pv_name, pv_values, mode=\"single\")` takes as parameters the `pv_name` in order to get the right scaling parameters from the structure where they are stored, the `pv_values` to scale in the form of a list, and the mode of scaling:\n",
    "\n",
    "-`single` scales PV independently from the others: thus computing min and max of the PV and scaling to the [0, 1] interval. This is apt to off-line scenarios\n",
    "- `group` scales the PVs with respect to values of the same PV in the reference batch. This way, the online and offline scenarios are treated equally from this point of view\n",
    "\n",
    "The scaling formula adopted is:\n",
    "$$X_{scaled} = \\frac{X_{original} - min}{max - min}$$\n",
    "where min and max have different meaning for `single` and `group` scaling. If a PV is constant in any one of the queries, it is scaled by default to the constant 0.5 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_pv(self, pv_name, pv_values, mode=\"single\"):\n",
    "        \"\"\"\n",
    "        Scales features in two possible ways:\n",
    "            'single': the feature is scaled according to the values it assumes in the current batch\n",
    "            'group': the feature is scaled according to its average range across the whole data set\n",
    "        \"\"\"\n",
    "        if mode == \"single\":\n",
    "            pv_min = min(pv_values)\n",
    "            pv_max = max(pv_values)\n",
    "            if abs(pv_max-pv_min) > 1e-6:\n",
    "                scaled_pv_values = (np.array(pv_values)-pv_min)/(pv_max-pv_min)\n",
    "            else:\n",
    "                scaled_pv_values = .5 * np.ones(len(pv_values))\n",
    "        elif mode == \"group\":\n",
    "            pv_min, pv_max = self.scale_params[pv_name]\n",
    "            scaled_pv_values = (np.array(pv_values)-pv_min)/(pv_max-pv_min)\n",
    "        return scaled_pv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`convert_to_mvts(batch)` takes as input a batch in the usual format (list of PV dictionaries) and turns it to a numpy array for faster computation. The values of each PV (columns of the numpy array) are scaled according to the method specified at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mvts(self, batch):     # mvts = Multi Variate Time Series\n",
    "        \"\"\"\n",
    "        Takes one batch in the usual form (list of one dictionary per PV) and transforms\n",
    "        it to a numpy array to perform calculations faster\n",
    "        \"\"\"\n",
    "        k = len(batch[0]['values'])  # Length of a batch (number of data points per single PV)\n",
    "        num_feat = len(batch)  # Number of PVs\n",
    "\n",
    "        mvts = np.zeros((k, num_feat))\n",
    "\n",
    "        for (i, pv_dict) in zip(np.arange(num_feat), batch):\n",
    "            mvts[:, i] = self.scale_pv(pv_dict['name'], pv_dict['values'], \"group\")\n",
    "\n",
    "        return mvts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW implementation\n",
    "`comp_dist_matrix(reference_ts, query_ts, n_jobs=1)` takes as input the reference and a query batch in the mvts form (returned by `convert_to_mvts(batch)`) and computes the local distance matrix using the eucliden distance measure. The `pairwise_distances` function takes the weight for the features from those stored in the `Dtw` object. `n_jobs` is used for parallelization (not always faster that single core computation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_dist_matrix(self, reference_ts, query_ts, n_jobs=1):\n",
    "        \"\"\"\n",
    "        Computes the distance matrix with ref_len (length of the reference) number of rows and\n",
    "        query_len (length of the query) number of columns (OK with convention on indices in dtw)\n",
    "        with dist_measure as local distance measure\n",
    "\n",
    "        reference_ts: mvts representation of reference batch\n",
    "        query_ts: mvts representation of query batch\n",
    "\n",
    "        n_jobs: number of jobs for pairwise_distances function. It could cause problems on windows\n",
    "        \"\"\"\n",
    "        _, d_1 = reference_ts.shape\n",
    "        _, d_2 = query_ts.shape\n",
    "\n",
    "        if d_1 != d_2:\n",
    "            print(\"Number of features not coherent between reference ({0}) and query ({1})\"\n",
    "                  .format(d_1, d_2))\n",
    "            return None\n",
    "\n",
    "        distance_matrix = pairwise_distances(\n",
    "            X=reference_ts, Y=query_ts, metric=euclidean, n_jobs=n_jobs, w=self.data['feat_weights']\n",
    "            )\n",
    "\n",
    "        return distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`comp_acc_dist_matrix(distance_matrix, step_pattern='symmetricP05', open_ended=False)` computes the accumulated distance matrix starting from the local distance matrix according to the specified step pattern. Probably the `open_ended` parameter is superfluous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_acc_dist_matrix(self, distance_matrix, step_pattern='symmetricP05', open_ended=False):\n",
    "        \"\"\"\n",
    "        Computes the accumulated distance matrix starting from the distance_matrix according to the\n",
    "        step_pattern indicated\n",
    "        distance_matrix: cross distance matrix\n",
    "        step_pattern: string indicating the step pattern to be used. Can be symmetric1/2,\n",
    "        symmetricP05 or symmetricPX, with X any positive integer\n",
    "        \"\"\"\n",
    "        ref_len, query_len = distance_matrix.shape\n",
    "        acc_dist_matrix = np.empty((ref_len, query_len))\n",
    "        if not open_ended:\n",
    "            for i in np.arange(ref_len):\n",
    "                for j in np.arange(query_len):\n",
    "                    acc_dist_matrix[i, j] = self.comp_acc_element(\n",
    "                        i, j, acc_dist_matrix, distance_matrix, step_pattern)\\\n",
    "                        if self.itakura(i, j, ref_len, query_len, step_pattern) else np.inf\n",
    "\n",
    "        else:\n",
    "            for i in np.arange(ref_len):\n",
    "                for j in np.arange(query_len):\n",
    "                    acc_dist_matrix[i, j] = self.comp_acc_element(\n",
    "                        i, j, acc_dist_matrix, distance_matrix, step_pattern)\n",
    "        return acc_dist_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`comp_acc_element(i, j, acc_dist_matrix, distance_matrix, step_pattern)` computes the element (i,j) of the accumulated distance matrix according to the specified step pattern. The possible step patterns are `symmetric1`, `symmetric2`, `symmetricP05` and `symmetricPN` for N positive integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_acc_element(self, i, j, acc_dist_matrix, distance_matrix, step_pattern):\n",
    "        \"\"\"\n",
    "        Computes the value of a cell of the accumulated distance matrix\n",
    "        i: row (reference) index\n",
    "        j: column (query) index\n",
    "        acc_dist_matrix: current accumulated distance matrix\n",
    "        distance_matrix: cross distance matrix\n",
    "        step_pattern: step pattern to be used for calculations\n",
    "        \"\"\"\n",
    "        if (i == 0 and j == 0):\n",
    "            return distance_matrix[0, 0]\n",
    "\n",
    "        if step_pattern == \"symmetricP05\":\n",
    "\n",
    "            p_1 = acc_dist_matrix[i-1, j-3] + 2 * distance_matrix[i, j-2] + distance_matrix[i, j-1]\\\n",
    "                + distance_matrix[i, j] if (i-1 >= 0 and j-3 >= 0) else np.inf\n",
    "            p_2 = acc_dist_matrix[i-1, j-2] + 2 * distance_matrix[i, j-1] + \\\n",
    "                distance_matrix[i, j] if (i-1 >= 0 and j-2 >= 0) else np.inf\n",
    "            p_3 = acc_dist_matrix[i-1, j-1] + 2 * \\\n",
    "                distance_matrix[i, j] if (i-1 >= 0 and j-1 >= 0) else np.inf\n",
    "            p_4 = acc_dist_matrix[i-2, j-1] + 2 * distance_matrix[i-1, j] + \\\n",
    "                distance_matrix[i, j] if (i-2 >= 0 and j-1 >= 0) else np.inf\n",
    "            p_5 = acc_dist_matrix[i-3, j-1] + 2 * distance_matrix[i-2, j] + distance_matrix[i-1, j]\\\n",
    "                + distance_matrix[i, j] if (i-3 >= 0 and j-1 >= 0) else np.inf\n",
    "\n",
    "            return min(p_1, p_2, p_3, p_4, p_5)  \n",
    "\n",
    "        if step_pattern == \"symmetric1\":\n",
    "            p_1 = acc_dist_matrix[i, j-1] + distance_matrix[i, j] if (j-1 >= 0) else np.inf\n",
    "            p_2 = acc_dist_matrix[i-1, j-1] + distance_matrix[i, j]\\\n",
    "                if (i-1 >= 0 and j-1 >= 0) else np.inf\n",
    "            p_3 = acc_dist_matrix[i-1, j] + distance_matrix[i, j] if (i-1 >= 0) else np.inf\n",
    "\n",
    "            return min(p_1, p_2, p_3)\n",
    "\n",
    "        if step_pattern == \"symmetric2\":\n",
    "            p_1 = acc_dist_matrix[i, j-1] + distance_matrix[i, j] if (j-1 >= 0) else np.inf\n",
    "            p_2 = acc_dist_matrix[i-1, j-1] + 2 * \\\n",
    "                distance_matrix[i, j] if (i-1 >= 0 and j-1 >= 0) else np.inf\n",
    "            p_3 = acc_dist_matrix[i-1, j] + distance_matrix[i, j] if (i-1 >= 0) else np.inf\n",
    "\n",
    "            return min(p_1, p_2, p_3)  \n",
    "\n",
    "        patt = re.compile(\"symmetricP[1-9]+\\d*\")\n",
    "        if patt.match(step_pattern):\n",
    "            p = int(step_pattern[10:])\n",
    "            p_1 = acc_dist_matrix[i-p, j-(p+1)] + 2*sum([distance_matrix[i-p, j-(p+1)]\\\n",
    "                                             for p in np.arange(0, p)]) + distance_matrix[i, j]\\\n",
    "                                                        if (i-p >= 0 and j-(p+1) >= 0) else np.inf\n",
    "            p_2 = acc_dist_matrix[i-1, j-1] + \\\n",
    "                2 * distance_matrix[i, j] if (i-1 >= 0 and j-1 >= 0) else np.inf\n",
    "            p_3 = acc_dist_matrix[i-(p+1), j-p] + 2*sum([distance_matrix[i-(p+1), j-p]\\\n",
    "                                             for p in np.arange(0, p)]) + distance_matrix[i, j] \\\n",
    "                                                                    if (i-(p+1) >= 0 and j-p >= 0) \\\n",
    "                                                                                        else np.inf\n",
    "\n",
    "            return min(p_1, p_2, p_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_warping_path(acc_dist_matrix, step_pattern, ref_len, query_len)` computes the warping path on the accumulated distance matrix, coherent with the given step pattern. `ref_len` and `query_len` specify the last point of the warping path, in order to make the computation possible in the open-ended version too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warping_path(self, acc_dist_matrix, step_pattern, ref_len, query_len):\n",
    "        \"\"\"\n",
    "        Computes the warping path on the acc_dist_matrix induced by step_pattern starting from\n",
    "        the (ref_len,query_len) point (this in order to use the method in both open_ended and global\n",
    "        alignment)\n",
    "        Return the warping path (list of tuples) in ascending order\n",
    "        \"\"\"\n",
    "        # ref_len, query_len = acc_dist_matrix.shape\n",
    "        warping_path = list()\n",
    "\n",
    "        if step_pattern == \"symmetric1\" or step_pattern == \"symmetric2\":\n",
    "            i = ref_len-1\n",
    "            j = query_len-1\n",
    "            while i != 0 or j != 0:\n",
    "                warping_path.append((i, j))\n",
    "                candidates = list()\n",
    "                if i > 0:\n",
    "                    candidates.append((acc_dist_matrix[i-1, j], (i-1, j)))\n",
    "                if j > 0:\n",
    "                    candidates.append((acc_dist_matrix[i, j-1], (i, j-1)))\n",
    "                if len(candidates) == 2:\n",
    "                    candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "\n",
    "                next_step = min(candidates)[1]\n",
    "                i, j = next_step\n",
    "            warping_path.append((0, 0))\n",
    "\n",
    "            return warping_path[::-1]\n",
    "\n",
    "        elif step_pattern == \"symmetricP05\":\n",
    "            # maxWarp = 2\n",
    "            # minDiag = 1\n",
    "            i = ref_len-1\n",
    "            j = query_len-1\n",
    "\n",
    "            if np.isnan(acc_dist_matrix[i, j]):\n",
    "                print(\"Invalid value for P, \\\n",
    "                      a global alignment is not possible with this local constraint\")\n",
    "                return\n",
    "            h_step = 0  # horizontal step\n",
    "            v_step = 0  # vertical step\n",
    "            d_step = 0  # diagonal step\n",
    "\n",
    "            while i != 0 or j != 0:\n",
    "                warping_path.append((i, j))\n",
    "                candidates = list()\n",
    "\n",
    "                if h_step > 0:\n",
    "                    if h_step == 1:\n",
    "                        if j > 0:\n",
    "                            candidates.append((acc_dist_matrix[i, j-1], (i, j-1)))\n",
    "                        if j > 0 and i > 0:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "                    elif h_step == 2:\n",
    "                        if j > 0 and i > 0:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "\n",
    "                elif v_step > 0:\n",
    "                    if v_step == 1:\n",
    "                        if i > 0:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j], (i-1, j)))\n",
    "                        if j > 0 and i > 0:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "                    elif v_step == 2:\n",
    "                        if j > 0 and i > 0:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "\n",
    "                else:\n",
    "                    if j > 0:\n",
    "                        candidates.append((acc_dist_matrix[i, j-1], (i, j-1)))\n",
    "                    if i > 0:\n",
    "                        candidates.append((acc_dist_matrix[i-1, j], (i-1, j)))\n",
    "                    if j > 0 and i > 0:\n",
    "                        candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "\n",
    "                next_step = min(candidates)[1]\n",
    "                v = next_step[0] < i\n",
    "                h = next_step[1] < j\n",
    "                d = v and h\n",
    "\n",
    "                if d:\n",
    "                    v_step = 0\n",
    "                    h_step = 0\n",
    "                elif v:\n",
    "                    v_step += 1\n",
    "                elif h:\n",
    "                    h_step += 1\n",
    "\n",
    "                i, j = next_step\n",
    "\n",
    "            warping_path.append((0, 0))\n",
    "\n",
    "            return warping_path[::-1]\n",
    "\n",
    "        else:\n",
    "            patt = re.compile(\"symmetricP[1-9]+\\d*\")\n",
    "            if patt.match(step_pattern):\n",
    "\n",
    "                min_diag_steps = int(step_pattern[10:])\n",
    "\n",
    "                warp_step = 0\n",
    "                d_step = 0\n",
    "                i = ref_len-1\n",
    "                j = query_len-1\n",
    "\n",
    "                if np.isinf(acc_dist_matrix[i, j]):\n",
    "                    print(\"Invalid value for P, \\\n",
    "                          a global alignment is not possible with this local constraint\")\n",
    "                    return\n",
    "\n",
    "                while i != 0 and j != 0:\n",
    "                    warping_path.append((i, j))\n",
    "                    candidates = list()\n",
    "                    if warp_step > 0:\n",
    "                        candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "                    else:\n",
    "                        if j > 0:\n",
    "                            candidates.append((acc_dist_matrix[i, j-1], (i, j-1)))\n",
    "                        if i > 0:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j], (i-1, j)))\n",
    "                        if len(candidates) == 2:\n",
    "                            candidates.append((acc_dist_matrix[i-1, j-1], (i-1, j-1)))\n",
    "\n",
    "                    next_step = min(candidates)[1]\n",
    "                    v = next_step[0] < i\n",
    "                    h = next_step[1] < j\n",
    "                    d = v and h\n",
    "\n",
    "                    if d:\n",
    "                        d_step += 1\n",
    "                        if d_step == min_diag_steps:\n",
    "                            d_step = 0\n",
    "                            warp_step = 0\n",
    "                        elif d_step < min_diag_steps and warp_step > 0:\n",
    "                            pass\n",
    "                        elif d_step < min_diag_steps and warp_step == 0:\n",
    "                            d_step = 0\n",
    "                    else:\n",
    "                        warp_step += 1\n",
    "\n",
    "                    i, j = next_step\n",
    "\n",
    "                warping_path.append((0, 0))\n",
    "\n",
    "                return warping_path[::-1]\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid step-pattern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`call_dtw(query_id, step_pattern=\"symmetricP05\", n_jobs=1, open_ended=False, get_results=False, length = 0, all_sub_seq=False)` calls the `dtw` method on the pair (reference, query) specified by `query_id`. `get_results` specifies if the method should return DTW distance, warping path and accumulated distance matrix as output or if it should just store the information inside the `Dtw` object. `length` refers to the query length to consider in case of open-ended DTW, and `all_sub_seq` computes the open-ended DTW for all the sub-sequences of the query series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_dtw(self, query_id, step_pattern=\"symmetricP05\",\n",
    "                 n_jobs=1, open_ended=False, get_results=False, length = 0, all_sub_seq=False):\n",
    "        \"\"\"\n",
    "        Calls the dtw method on the data stored in the .data attribute (needs only the query_id in \\\n",
    "        addition to standard parameters)\n",
    "        get_results if True returns the distance and the warping calculated; if False, \\\n",
    "        only the .data attribute is updated\n",
    "        \"\"\"\n",
    "        if not open_ended:\n",
    "            if step_pattern in self.data['warpings_per_step_pattern']:\n",
    "                if query_id in self.data['warpings_per_step_pattern'][step_pattern]:\n",
    "                    return\n",
    "\n",
    "            reference_ts = self.convert_to_mvts(self.data['reference'])\n",
    "            query_ts = self.convert_to_mvts(self.data['queries'][query_id])\n",
    "\n",
    "            result = self.dtw(reference_ts, query_ts, step_pattern, n_jobs, open_ended)\n",
    "\n",
    "            self.data[\"warpings\"][query_id] = result[\"warping\"]\n",
    "            self.data[\"distances\"][query_id] = result[\"DTW_distance\"]\n",
    "            self.data['warp_dist'][query_id]=list()\n",
    "            for (i,j) in result[\"warping\"]:\n",
    "                self.data['warp_dist'][query_id].append((i, j, result['acc_matrix'][i, j]/(i+j+2)))\n",
    "\n",
    "            self.data['time_distortion'][step_pattern][query_id] = \\\n",
    "                self.time_distortion(result['warping'])\n",
    "            self.data['distance_distortion'][step_pattern][query_id] = result[\"DTW_distance\"]\n",
    "            self.data['warpings_per_step_pattern'][step_pattern][query_id] = result['warping']\n",
    "\n",
    "            if get_results:\n",
    "                return result\n",
    "\n",
    "        if open_ended:\n",
    "            # ADD ALL SUB SEQUENCE OPTION#############################\n",
    "            if all_sub_seq:\n",
    "                reference_ts = self.convert_to_mvts(self.data['reference'])\n",
    "                query_ts = self.convert_to_mvts(self.data['queries'][query_id])\n",
    "\n",
    "                result = self.dtw(reference_ts, query_ts, step_pattern, n_jobs, open_ended=False)\n",
    "\n",
    "                acc_dist_matrix = result['acc_matrix']\n",
    "                N, M = acc_dist_matrix.shape\n",
    "                self.data_open_ended['warp_dist'][query_id] = list()\n",
    "                for j in np.arange(M):\n",
    "                    candidate = acc_dist_matrix[:, j]\n",
    "                    candidate /= np.arange(2, N+2) + j\n",
    "                    i_min, dtw_dist = np.argmin(candidate), min(candidate)\n",
    "\n",
    "                    self.data_open_ended['warp_dist'][query_id].append((i_min, j, dtw_dist))\n",
    "                return\n",
    "\n",
    "            if not length:\n",
    "                print(\"Length cannot be 0\")\n",
    "                return\n",
    "\n",
    "            if not self.check_open_ended(query_id, length, step_pattern):\n",
    "\n",
    "                query_ts = self.convert_to_mvts(self.online_query(query_id, length))\n",
    "                reference_ts = self.convert_to_mvts(self.data['reference'])\n",
    "\n",
    "                result = self.dtw(reference_ts, query_ts, step_pattern, n_jobs, open_ended)\n",
    "\n",
    "                data_point = {'length': length,\n",
    "                              'DTW_distance': result['DTW_distance'],\n",
    "                              'warping':result['warping'],\n",
    "                              'step_pattern': step_pattern}\n",
    "                self.data_open_ended['queries'][query_id].append(data_point)\n",
    "\n",
    "            if get_results:\n",
    "                return list(filter(lambda x: x['step_pattern']==step_pattern and x['length']==length, self.data_open_ended['queries'][query_id]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`dtw(reference_ts, query_ts, step_pattern=\"symmetricP05\", n_jobs=1, open_ended=False)` is the method that actually performs the DTE computation. It accepts series in the mvts format, and in case of standard (not open-ended) version, checks that the step pattern allows for a global alignment. The output is a dictionary containing the DTW distance, the warping path and the accumulated distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw(self, reference_ts, query_ts, step_pattern=\"symmetricP05\",\n",
    "            n_jobs=1, open_ended=False):\n",
    "        \"\"\"\n",
    "        Compute alignment betwwen reference_ts and query_ts (already in mvts form).\n",
    "        Separate from call_dtw() for testing purposes\n",
    "        \"\"\"\n",
    "        # Check for coherence of local constraint and global alignment\n",
    "        # (in case a PX local constraint is used)\n",
    "        if not open_ended:\n",
    "            patt = re.compile(\"symmetricP[1-9]+\\d*\")\n",
    "            if patt.match(step_pattern):\n",
    "                p = int(step_pattern[step_pattern.index(\"P\")+1:])\n",
    "                ref_len, query_len = len(reference_ts), len(query_ts)\n",
    "                p_max = np.floor(min(ref_len, query_len)/np.abs(ref_len-query_len)) \\\n",
    "                    if np.abs(ref_len-query_len) > 0 else np.inf\n",
    "                if p > p_max:\n",
    "                    print(\"Invalid value for P, \\\n",
    "                                  a global alignment is not possible with this local constraint\")\n",
    "                    return\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        distance_matrix = self.comp_dist_matrix(reference_ts, query_ts, n_jobs)\n",
    "\n",
    "        acc_dist_matrix = self.comp_acc_dist_matrix(distance_matrix, step_pattern, open_ended)\n",
    "\n",
    "        ref_len, query_len = acc_dist_matrix.shape\n",
    "        # In case of open-ended version\n",
    "        # correctly identifies the starting point on the reference batch for warping\n",
    "        if open_ended:\n",
    "            ref_len = self.get_ref_prefix_length(acc_dist_matrix)\n",
    "\n",
    "        warping = self.get_warping_path(acc_dist_matrix, step_pattern, ref_len, query_len)\n",
    "\n",
    "        dtw_dist = acc_dist_matrix[ref_len-1, query_len-1] / (ref_len+query_len) if step_pattern != \"symmetric1\" \\\n",
    "                                                                    else acc_dist_matrix[ref_len-1, query_len-1]\n",
    "\n",
    "        return {\"warping\": warping,\n",
    "                \"DTW_distance\": dtw_dist,\n",
    "                'acc_matrix': acc_dist_matrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_ref_prefix_length(acc_dist_matrix)` This method is called in case of open-ended DTW: it computes the length of the reference prefix to which the query is mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_prefix_length(self, acc_dist_matrix):\n",
    "        \"\"\"\n",
    "        Computes the length of the reference prefix in case of open-ended alignment\n",
    "        \"\"\"\n",
    "        N, M = acc_dist_matrix.shape\n",
    "        last_column = acc_dist_matrix[:, -1]/np.arange(1, N+1)\n",
    "\n",
    "        ref_prefix_len = np.argmin(last_column) + 1\n",
    "        return ref_prefix_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`itakura(i, j, ref_len, query_len, step_pattern)` checks if the accumulated distance matrix cell in position (i,j) is or is not inside the Itakura parallelogram that the step pattern (of the `symmetricPN` class) imposes to the accumulated distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itakura(self, i, j, ref_len, query_len, step_pattern):\n",
    "        \"\"\"\n",
    "        Induced Itakura global constraint for GLOBAL ALIGNMENT\n",
    "        \"\"\"\n",
    "        patt = re.compile(\"symmetricP[1-9]+\\d*\")\n",
    "        if step_pattern == \"symmetricP05\":\n",
    "            p = 1/2\n",
    "        elif patt.match(step_pattern):\n",
    "            p = int(step_pattern[step_pattern.index('P')+1:])\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "        in_domain = (i >= np.floor(j*p/(p+1))) and \\\n",
    "                    (i <= np.ceil(j*(p+1)/p)) and \\\n",
    "                    (i <= np.ceil(ref_len+(j-query_len)*(p/(p+1)))) and \\\n",
    "                    (i >= np.floor(ref_len+(j-query_len)*((p+1)/p)))\n",
    "        return in_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`extreme_itakura(i, j, ref_len, query_len, step_pattern)` alternative implementation of the itakura method, could speed up the computation in some settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extreme_itakura(self, i, j, ref_len, query_len, step_pattern):\n",
    "        \"\"\"\n",
    "        Alternative implementation of itakura method\n",
    "        \"\"\"\n",
    "        case = 0\n",
    "        patt = re.compile(\"symmetricP[1-9]+\\d*\")\n",
    "        if step_pattern == \"symmetricP05\":\n",
    "            p = 1/2\n",
    "        elif patt.match(step_pattern):\n",
    "            p = int(step_pattern[step_pattern.index('P')+1:])\n",
    "        else:\n",
    "            return (case, True)\n",
    "\n",
    "        if (i < np.floor(j*p/(p+1))) or (i < np.floor(ref_len+(j-query_len)*((p+1)/p))):\n",
    "            case = 1\n",
    "            return (case, False)\n",
    "\n",
    "        in_domain = (i >= np.floor(j*p/(p+1))) and \\\n",
    "                    (i <= np.ceil(j*(p+1)/p)) and \\\n",
    "                    (i <= np.ceil(ref_len+(j-query_len)*(p/(p+1)))) and \\\n",
    "                    (i >= np.floor(ref_len+(j-query_len)*((p+1)/p)))\n",
    "\n",
    "        return (case, in_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`check_open_ended(query_id, length, step_pattern)` check if the open-ended DTW has already been computed, in order to avoid needless computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_open_ended(self, query_id, length, step_pattern):\n",
    "        check_id = query_id in self.data_open_ended['queries']\n",
    "        if check_id:\n",
    "            check = bool(list(filter(\n",
    "                lambda x: x['step_pattern'] == step_pattern and x['length'] == length, self.data_open_ended['queries'][query_id])))\n",
    "            return check\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step pattern selection utilities\n",
    "\n",
    "`time_distortion(warping_path)` computes the time distortion of an alignment, counting the number of warping steps (non diagonal steps) the warping path contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distortion(self, warping_path):\n",
    "        \"\"\"\n",
    "        Computes the time distortion caused by warping_path\n",
    "        \"\"\"\n",
    "        T = len(warping_path)\n",
    "        f_q = [w[1] for w in warping_path]\n",
    "        f_r = [w[0] for w in warping_path]\n",
    "\n",
    "        t_d = [(f_r[t+1] - f_r[t])*(f_q[t+1] - f_q[t]) == 0 for t in np.arange(T-1)]\n",
    "\n",
    "        return sum(t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`avg_time_distortion(step_pattern)` computes the average time distortion, relative to a certain step pattern, for the whole data set of queries contained in the `Dtw` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_time_distortion(self, step_pattern):\n",
    "        \"\"\"\n",
    "        Computes the average time distortion relative to a certain step pattern\n",
    "        \"\"\"\n",
    "        if len(self.data['time_distortion'][step_pattern]) != self.data['num_queries']:\n",
    "            print('Not every query aligned, align the remaining queries')\n",
    "            return\n",
    "        else:\n",
    "            I = self.data['num_queries']\n",
    "            avg_td = sum(self.data['time_distortion'][step_pattern].values())/I\n",
    "\n",
    "            return avg_td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`avg_distance(step_pattern)` computes the average DTW distance, relative to a certain step pattern, for the whole data set of queries contained in the `Dtw` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_distance(self, step_pattern):\n",
    "        \"\"\"\n",
    "        Computes average\n",
    "        \"\"\"\n",
    "        if len(self.data['distance_distortion'][step_pattern]) != self.data['num_queries']:\n",
    "            print('Not every query aligned, align the remaining queries')\n",
    "            return\n",
    "        else:\n",
    "            I = self.data['num_queries']\n",
    "            avg_dist = sum(self.data['distance_distortion'][step_pattern].values())/I\n",
    "\n",
    "            return avg_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_p_max(query_id)` computes the maximum value of the parameter P (for `symmetricPN` step patterns) that allows for a global alignment between the reference and the query with ID `query_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_max(self, query_id):\n",
    "        \"\"\"\n",
    "        Computes the maximum value of P for the selected query batch\n",
    "        \"\"\"\n",
    "        k_q = len(self.data['queries'][query_id][0]['values'])\n",
    "        k_r = len(self.data['reference'][0]['values'])\n",
    "        p_max = np.floor(min(k_q, k_r)/abs(k_q - k_r)) if abs(k_q - k_r) > 0 else k_r\n",
    "        return p_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_global_p_max():` computes the maximum value of the parameter P (for `symmetricPN` step patterns) that allows for a global alignment for every query in the `Dtw` object. This correspond to the minimum of the set of P-max for all the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_p_max(self):\n",
    "        \"\"\"\n",
    "        Computes the maximum value of P for the data set under consideration\n",
    "        \"\"\"\n",
    "        p_maxs = [self.get_p_max(query_id) for query_id in self.data['queriesID']]\n",
    "        return int(min(p_maxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables weights optimization\n",
    "- `reset_weights`\n",
    "- `compute_mld`\n",
    "- `extract_single_feat\n",
    "- `weight_optimization_single_batch`\n",
    "- `weight_optimization_step`\n",
    "- `optimize_weights`\n",
    "- `get_weight_variables`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(self, random=False):\n",
    "        \"\"\"\n",
    "        Reset the variables' weights to 1 or to random [0,1] numbers\n",
    "        \"\"\"\n",
    "        n_feat = len(self.data['reference'])\n",
    "        if not random:\n",
    "            weights = np.ones(n_feat)\n",
    "        else:\n",
    "            weights = np.abs(np.random.normal(loc=1.0, size=n_feat))\n",
    "            weights = weights/sum(weights) * n_feat\n",
    "\n",
    "        self.data['feat_weights'] = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visualization\n",
    "- `distance_cost_plot`\n",
    "- `plot_weights`\n",
    "- `plot_by_name`\n",
    "- `do_warp`\n",
    "- `plot_warped_curves`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### misc\n",
    "- `online_scale`\n",
    "- `online_query`\n",
    "- `generate_train_set`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

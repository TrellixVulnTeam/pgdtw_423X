{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the various elements of the DTW component of the predictive system. The actual module is stored in the `libdtw.py` file.\n",
    "\n",
    "## Preliminaries: Data Loading\n",
    "The `libdtw.py` module contains 2 functions:\n",
    "- `load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\")`\n",
    "- `assign_ref(data)`\n",
    "\n",
    "and one class:\n",
    "- `Dtw(json_obj = False)`\n",
    "\n",
    "We first illustrate the usage of the two funcitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\")` load the .pickle in the `data_path` position. This has to be a dictionary where the keys are the batch IDs and the values are lists of dictionaries representing the PVs (with keys `name, start, end, values`). \n",
    "\n",
    "The function first identifies the median (with respect to the duration parameter) batch to be used as reference. Then, it selects the first `n_to_keep` batches closer to the reference one in terms of duration. The output dictionary has the `reference` key explicitly declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\"):\n",
    "    \"\"\"\n",
    "    Load data of operation 3.26, only the n_to_keep batches with duration closer to the median one\n",
    "    are selected\n",
    "    \"\"\"\n",
    "    with open(data_path, \"rb\") as infile:\n",
    "        data = pickle.load(infile)\n",
    "\n",
    "    operation_length = list()\n",
    "    pv_dataset = list()\n",
    "    for _id, pvs in data.items():\n",
    "        operation_length.append((len(pvs[0]['values']), _id))\n",
    "        pv_list = list()\n",
    "        for pv_dict in pvs:\n",
    "            pv_list.append(pv_dict['name'])\n",
    "        pv_dataset.append(pv_list)\n",
    "\n",
    "    median_len = np.median([l for l, _id in operation_length])\n",
    "\n",
    "    # Select the batches closer to the median bacth\n",
    "    # center around the median\n",
    "    centered = [(abs(l-median_len), _id) for l, _id in operation_length]\n",
    "    selected = sorted(centered)[:n_to_keep]\n",
    "\n",
    "    med_id = selected[0][1]\n",
    "\n",
    "    all_ids = list(data.keys())\n",
    "    for _id in all_ids:\n",
    "        if _id not in [x[1] for x in selected]:\n",
    "            _ = data.pop(_id)\n",
    "\n",
    "    data['reference'] = med_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assign_ref(data)` computes the median batch (acording to the duration parameter) and sets it as `reference` of the data set. It is useful in case the data loaded with `load_data()` undergoes modification prior to its actual use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ref(data):\n",
    "    data = copy(data)\n",
    "    operation_length = list()\n",
    "    pv_dataset = list()\n",
    "    for _id, pvs in data.items():\n",
    "        operation_length.append((len(pvs[0]['values']), _id))\n",
    "        pv_list = list()\n",
    "        for pv_dict in pvs:\n",
    "            pv_list.append(pv_dict['name'])\n",
    "        pv_dataset.append(pv_list)\n",
    "\n",
    "    median_len = np.median([l for l, _id in operation_length])\n",
    "\n",
    "    # Select the ref_len=50 closest to the median bacthes\n",
    "    # center around the median\n",
    "    centered = [(abs(l-median_len), _id) for l, _id in operation_length]\n",
    "    selected = sorted(centered)\n",
    "\n",
    "    med_id = selected[0][1]  # 5153\n",
    "\n",
    "    all_ids = list(data.keys())\n",
    "    for _id in all_ids:\n",
    "        if _id not in [x[1] for x in selected]:\n",
    "            _ = data.pop(_id)\n",
    "\n",
    "    data['reference'] = med_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dtw class\n",
    "The `Dtw(json_obj)` contains all the methods used to set up the DTW algorithm, optimizing the variables weights, computing the alignment. it contains the following methods (divided by topic here for clarity):\n",
    "\n",
    "##### data handling\n",
    "- `__init__()`\n",
    "- `convert_data_from_json()`\n",
    "- `add_query`\n",
    "- `get_scaling_parameters`\n",
    "- `remove_const_feats`\n",
    "- `scale_pv`\n",
    "- `convert_to_mvts`\n",
    "\n",
    "##### dtw implementation\n",
    "- `comp_dist_matrix`\n",
    "- `comp_acc_dist_matrix`\n",
    "- `comp_acc_element`\n",
    "- `get_warping_path`\n",
    "- `call_dtw`\n",
    "- `dtw`\n",
    "- `get_ref_prefix_length`\n",
    "- `itakura`\n",
    "- `extreme_itakura`\n",
    "- `check_open_ended`\n",
    "\n",
    "##### step pattern selection utilities\n",
    "- `time_distortion`\n",
    "- `avg_time_distortion`\n",
    "- `avg_distance`\n",
    "- `get_p_max`\n",
    "- `get_global_p_max`\n",
    "\n",
    "##### variables weights optimization\n",
    "- `reset_weights`\n",
    "- `compute_mld`\n",
    "- `extract_single_feat\n",
    "- `weight_optimization_single_batch`\n",
    "- `weight_optimization_step`\n",
    "- `optimize_weights`\n",
    "- `get_weight_variables`\n",
    "\n",
    "##### visualization\n",
    "- `distance_cost_plot`\n",
    "- `plot_weights`\n",
    "- `plot_by_name`\n",
    "- `do_warp`\n",
    "- `plot_warped_curves`\n",
    "\n",
    "##### misc\n",
    "- `online_scale`\n",
    "- `online_query`\n",
    "- `generate_train_set`\n",
    "\n",
    "We now examine each method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling\n",
    "`__init__(json_obj=False, random_weights = True, scaling='group')` initialize the `Dtw` class. \n",
    "\n",
    "- `json_obj` is the dictionary containing the data in the output format of `load_data()`. \n",
    "- `random_weights` if True, initialize the variable weights to randomly chosen values in the [0.1, 1] interval. \n",
    "- `scaling` is the scaling strategy for the PVs: `group` scales the PVs according to the values of the PVs in the reference batch, `single` scales the PVs as individual entities\n",
    "\n",
    "The initialization consists in structuring the data inside the Dtw object, removing the constant features (in the reference batch) filtering the batches that does not contain all the PVs of the reference batch, and finally setting the variables weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, json_obj=False, random_weights = True, scaling='group'):\n",
    "    \"\"\"\n",
    "    Initialization of the class.\n",
    "    json_obj: contains the data in the usual format\n",
    "    \"\"\"\n",
    "    if not json_obj:\n",
    "        pass\n",
    "    else:\n",
    "        self.convert_data_from_json(deepcopy(json_obj))\n",
    "        #self.scale_params = self.get_scaling_parameters()\n",
    "        self.remove_const_feats()\n",
    "        self.reset_weights(random=random_weights)\n",
    "        self.scaling = scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`convert_data_from_json(json_obj)` takes as input the data object specified before and separates it into reference and query batches. It also initialize the internal structure of the `Dtw` object. This includes the structure to collect data useful in subsequent operations and avoid repeating calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_from_json(self, json_obj):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing all the data, organized as:\n",
    "        ref_id: the ID of the reference batch\n",
    "        reference: reference batch in the usual format (list of dictionaries)\n",
    "        queries: list of dictionaries in which the keys are the query batch's ID and the values are\n",
    "        the actual batches (list of dictionaries)\n",
    "        num_queries: number of query batches in the data set\n",
    "        \"\"\"\n",
    "        ref_id = json_obj[\"reference\"]\n",
    "        reference = json_obj[ref_id]\n",
    "        queries = {key: batch for key, batch in json_obj.items() if key !=\n",
    "                   \"reference\" and key != ref_id}\n",
    "\n",
    "        self.data = {\"ref_id\": ref_id,\n",
    "                     \"reference\": reference,\n",
    "                     \"queries\": queries,\n",
    "                     \"num_queries\": len(queries),\n",
    "                     \"warpings\": dict(),\n",
    "                     \"distances\": dict(),\n",
    "                     'warp_dist': dict(),\n",
    "                     \"queriesID\": list(queries.keys()),\n",
    "                     \"time_distortion\": defaultdict(dict),\n",
    "                     \"distance_distortion\": defaultdict(dict),\n",
    "                     'warpings_per_step_pattern': defaultdict(dict),\n",
    "                     'feat_weights': 1.0}\n",
    "\n",
    "        self.data_open_ended = {\"ref_id\": ref_id,\n",
    "                                \"reference\": reference,\n",
    "                                \"queries\": defaultdict(list),\n",
    "                                'warp_dist': dict()}\n",
    "        scale_params = dict()\n",
    "\n",
    "        for pv_dict in self.data['reference']:\n",
    "            pv_name = pv_dict['name']\n",
    "            pv_min = min(pv_dict['values'])\n",
    "            pv_max = max(pv_dict['values'])\n",
    "            scale_params[pv_name] = (pv_min, pv_max)\n",
    "\n",
    "        self.scale_params = scale_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`add_query(batch_dict)` adds a batch (in the forms of a dict(batch_id: list_of_PVs)) to the `Dtw` object, updating the relevant structures and filtering the PVs. If the batch does not contain all the necessary PVs, it is not added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_query(self, batch_dict):\n",
    "    _id, pvs = list(batch_dict.items())[0]\n",
    "    self.data['queries'][_id] = pvs\n",
    "    self.data['num_queries'] += 1\n",
    "    self.data['queriesID'].append(_id)\n",
    "    self.remove_const_feats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`get_scaling_parameters()` probably can be omitted, TO CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling_parameters(self):\n",
    "        \"\"\"\n",
    "        Computes the parameters necessary for scaling the features as a 'group'.\n",
    "        This means considering the mean range of a variable across al the data set.\n",
    "        This seems creating problems, since the distributions for the minimum and the\n",
    "        maximum are too spread out. This method is here just in case of future use and to help\n",
    "        removing non-informative (constant) features.\n",
    "        avg_range = [avg_min, avg_max]\n",
    "        \"\"\"\n",
    "        scale_params = dict()\n",
    "\n",
    "        for pv_dict in self.data['reference']:\n",
    "            pv_name = pv_dict['name']\n",
    "            pv_min = min(pv_dict['values'])\n",
    "            pv_max = max(pv_dict['values'])\n",
    "\n",
    "            scale_params[pv_name] = [[pv_min], [pv_max]]\n",
    "\n",
    "        for _id, batch in self.data['queries'].items():\n",
    "            for pv_dict in batch:\n",
    "                pv_name = pv_dict['name']\n",
    "                pv_min = min(pv_dict['values'])\n",
    "                pv_max = max(pv_dict['values'])\n",
    "\n",
    "                scale_params[pv_name][0].append(pv_min)\n",
    "                scale_params[pv_name][1].append(pv_max)\n",
    "\n",
    "        pv_names = scale_params.keys()\n",
    "        for pv_name in pv_names:\n",
    "            scale_params[pv_name] = np.median(scale_params[pv_name], axis=1)\n",
    "\n",
    "        return scale_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`remove_const_feats()` removes the features that are constant in the reference batch from all the batches, reference and queries. If a query batch does not contain all the PVs of the filtered reference one, it is removed from the `Dtw` object. \n",
    "\n",
    "If a Pv is constant in the reference batch, this does not mean that it is constant also in any of the queries, but since all the DTW alignment are performed with respect to one single reference, this PVs would add no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_const_feats(self):\n",
    "        \"\"\"\n",
    "        Removes non-informative features (features with low variability)\n",
    "        \"\"\"\n",
    "        const_feats = list()\n",
    "        for pv_name, avg_range in self.scale_params.items():\n",
    "            if abs(avg_range[0]-avg_range[1]) < 1e-6:\n",
    "                const_feats.append(pv_name)\n",
    "        #const_feats.append('ba_TCzWpXo')\n",
    "        #const_feats.append('ba_TCfg3Yxn')\n",
    "        #const_feats.append('ba_FQYXdr6Q0')\n",
    "\n",
    "\n",
    "        initial_queries = list(self.data['queries'].keys())\n",
    "        print('Number of queries before filtering: %d'%len(initial_queries))\n",
    "\n",
    "        self.data['reference'] = list(filter(lambda x: x['name'] not in const_feats, self.data['reference']))\n",
    "        pv_names = [pv['name'] for pv in self.data['reference']]\n",
    "        for _id in initial_queries:\n",
    "            self.data['queries'][_id] = list(filter(lambda x: x['name']  in pv_names, self.data['queries'][_id]))\n",
    "            if len(self.data['queries'][_id]) != len(self.data['reference']):\n",
    "                _ = self.data['queries'].pop(_id)\n",
    "        print('Number of queries after filtering: %d'%len(self.data['queries']))\n",
    "\n",
    "        self.data['num_queries'] = len(self.data['queries'])\n",
    "        self.data['queriesID'] = list(self.data['queries'].keys())\n",
    "        self.pv_names = pv_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`scale_pv(pv_name, pv_values, mode=\"single\")` takes as parameters the `pv_name` in order to get the right scaling parameters from the structure where they are stored, the `pv_values` to scale in the form of a list, and the mode of scaling:\n",
    "\n",
    "-`single` scales PV independently from the others: thus computing min and max of the PV and scaling to the [0, 1] interval. This is apt to off-line scenarios\n",
    "- `group` scales the PVs with respect to values of the same PV in the reference batch. This way, the online and offline scenarios are treated equally from this point of view\n",
    "\n",
    "The scaling formula adopted is:\n",
    "$$X_{scaled} = \\frac{X_{original} - min}{max - min}$$\n",
    "where min and max have different meaning for `single` and `group` scaling. If a PV is constant in any one of the queries, it is scaled by default to the constant 0.5 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_pv(self, pv_name, pv_values, mode=\"single\"):\n",
    "        \"\"\"\n",
    "        Scales features in two possible ways:\n",
    "            'single': the feature is scaled according to the values it assumes in the current batch\n",
    "            'group': the feature is scaled according to its average range across the whole data set\n",
    "        \"\"\"\n",
    "        if mode == \"single\":\n",
    "            pv_min = min(pv_values)\n",
    "            pv_max = max(pv_values)\n",
    "            if abs(pv_max-pv_min) > 1e-6:\n",
    "                scaled_pv_values = (np.array(pv_values)-pv_min)/(pv_max-pv_min)\n",
    "            else:\n",
    "                scaled_pv_values = .5 * np.ones(len(pv_values))\n",
    "        elif mode == \"group\":\n",
    "            pv_min, pv_max = self.scale_params[pv_name]\n",
    "            scaled_pv_values = (np.array(pv_values)-pv_min)/(pv_max-pv_min)\n",
    "        return scaled_pv_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

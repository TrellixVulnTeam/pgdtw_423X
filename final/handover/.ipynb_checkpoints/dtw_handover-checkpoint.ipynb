{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the various elements of the DTW component of the predictive system. The actual module is stored in the `libdtw.py` file.\n",
    "\n",
    "## Preliminaries: Data Loading\n",
    "The `libdtw.py` module contains 2 functions:\n",
    "- `load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\")`\n",
    "- `assign_ref(data)`\n",
    "\n",
    "and one class:\n",
    "- `Dtw(json_obj = False)`\n",
    "\n",
    "We first illustrate the usage of the two funcitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\")` load the .pickle in the `data_path` position. This has to be a dictionary where the keys are the batch IDs and the values are lists of dictionaries representing the PVs (with keys `name, start, end, values`). \n",
    "\n",
    "The function first identifies the median (with respect to the duration parameter) batch to be used as reference. Then, it selects the first `n_to_keep` batches closer to the reference one in terms of duration. The output dictionary has the `reference` key explicitly declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_to_keep=50, data_path = \"data/ope3_26.pickle\"):\n",
    "    \"\"\"\n",
    "    Load data of operation 3.26, only the n_to_keep batches with duration closer to the median one\n",
    "    are selected\n",
    "    \"\"\"\n",
    "    with open(data_path, \"rb\") as infile:\n",
    "        data = pickle.load(infile)\n",
    "\n",
    "    operation_length = list()\n",
    "    pv_dataset = list()\n",
    "    for _id, pvs in data.items():\n",
    "        operation_length.append((len(pvs[0]['values']), _id))\n",
    "        pv_list = list()\n",
    "        for pv_dict in pvs:\n",
    "            pv_list.append(pv_dict['name'])\n",
    "        pv_dataset.append(pv_list)\n",
    "\n",
    "    median_len = np.median([l for l, _id in operation_length])\n",
    "\n",
    "    # Select the batches closer to the median bacth\n",
    "    # center around the median\n",
    "    centered = [(abs(l-median_len), _id) for l, _id in operation_length]\n",
    "    selected = sorted(centered)[:n_to_keep]\n",
    "\n",
    "    med_id = selected[0][1]\n",
    "\n",
    "    all_ids = list(data.keys())\n",
    "    for _id in all_ids:\n",
    "        if _id not in [x[1] for x in selected]:\n",
    "            _ = data.pop(_id)\n",
    "\n",
    "    data['reference'] = med_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assign_ref(data)` computes the median batch (acording to the duration parameter) and sets it as `reference` of the data set. It is useful in case the data loaded with `load_data()` undergoes modification prior to its actual use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ref(data):\n",
    "    data = copy(data)\n",
    "    operation_length = list()\n",
    "    pv_dataset = list()\n",
    "    for _id, pvs in data.items():\n",
    "        operation_length.append((len(pvs[0]['values']), _id))\n",
    "        pv_list = list()\n",
    "        for pv_dict in pvs:\n",
    "            pv_list.append(pv_dict['name'])\n",
    "        pv_dataset.append(pv_list)\n",
    "\n",
    "    median_len = np.median([l for l, _id in operation_length])\n",
    "\n",
    "    # Select the ref_len=50 closest to the median bacthes\n",
    "    # center around the median\n",
    "    centered = [(abs(l-median_len), _id) for l, _id in operation_length]\n",
    "    selected = sorted(centered)\n",
    "\n",
    "    med_id = selected[0][1]  # 5153\n",
    "\n",
    "    all_ids = list(data.keys())\n",
    "    for _id in all_ids:\n",
    "        if _id not in [x[1] for x in selected]:\n",
    "            _ = data.pop(_id)\n",
    "\n",
    "    data['reference'] = med_id\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dtw class\n",
    "The `Dtw(json_obj)` contains all the methods used to set up the DTW algorithm, optimizing the variables weights, computing the alignment. it contains the following methods (divided by topic here for clarity):\n",
    "\n",
    "##### data handling\n",
    "- `__init__()`\n",
    "- `convert_data_from_json()`\n",
    "- `add_query`\n",
    "- `get_scaling_parameters`\n",
    "- `remove_const_feats`\n",
    "- `scale_pv`\n",
    "- `convert_to_mvts`\n",
    "\n",
    "##### dtw implementation\n",
    "- `comp_dist_matrix`\n",
    "- `comp_acc_dist_matrix`\n",
    "- `comp_acc_element`\n",
    "- `get_warping_path`\n",
    "- `call_dtw`\n",
    "- `dtw`\n",
    "- `get_ref_prefix_length`\n",
    "- `itakura`\n",
    "- `extreme_itakura`\n",
    "- `check_open_ended`\n",
    "\n",
    "##### step pattern selection utilities\n",
    "- `time_distortion`\n",
    "- `avg_time_distortion`\n",
    "- `avg_distance`\n",
    "- `get_p_max`\n",
    "- `get_global_p_max`\n",
    "\n",
    "##### variables weights optimization\n",
    "- `reset_weights`\n",
    "- `compute_mld`\n",
    "- `extract_single_feat\n",
    "- `weight_optimization_single_batch`\n",
    "- `weight_optimization_step`\n",
    "- `optimize_weights`\n",
    "- `get_weight_variables`\n",
    "\n",
    "##### visualization\n",
    "- `distance_cost_plot`\n",
    "- `plot_weights`\n",
    "- `plot_by_name`\n",
    "- `do_warp`\n",
    "- `plot_warped_curves`\n",
    "\n",
    "##### misc\n",
    "- `online_scale`\n",
    "- `online_query`\n",
    "- `generate_train_set`\n",
    "\n",
    "We now examine each method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling\n",
    "`__init__(json_obj=False, random_weights = True, scaling='group')` initialize the `Dtw` class. \n",
    "\n",
    "- `json_obj` is the dictionary containing the data in the output format of `load_data()`. \n",
    "- `random_weights` if True, initialize the variable weights to randomly chosen values in the [0.1, 1] interval. \n",
    "- `scaling` is the scaling strategy for the PVs: `group` scales the PVs according to the values of the PVs in the reference batch, `single` scales the PVs as individual entities\n",
    "\n",
    "The initialization consists in structuring the data inside the Dtw object, removing the constant features (in the reference batch) filtering the batches that does not contain all the PVs of the reference batch, and finally setting the variables weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, json_obj=False, random_weights = True, scaling='group'):\n",
    "    \"\"\"\n",
    "    Initialization of the class.\n",
    "    json_obj: contains the data in the usual format\n",
    "    \"\"\"\n",
    "    if not json_obj:\n",
    "        pass\n",
    "    else:\n",
    "        self.convert_data_from_json(deepcopy(json_obj))\n",
    "        #self.scale_params = self.get_scaling_parameters()\n",
    "        self.remove_const_feats()\n",
    "        self.reset_weights(random=random_weights)\n",
    "        self.scaling = scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_from_json(self, json_obj):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing all the data, organized as:\n",
    "        ref_id: the ID of the reference batch\n",
    "        reference: reference batch in the usual format (list of dictionaries)\n",
    "        queries: list of dictionaries in which the keys are the query batch's ID and the values are\n",
    "        the actual batches (list of dictionaries)\n",
    "        num_queries: number of query batches in the data set\n",
    "        \"\"\"\n",
    "        ref_id = json_obj[\"reference\"]\n",
    "        reference = json_obj[ref_id]\n",
    "        queries = {key: batch for key, batch in json_obj.items() if key !=\n",
    "                   \"reference\" and key != ref_id}\n",
    "\n",
    "        self.data = {\"ref_id\": ref_id,\n",
    "                     \"reference\": reference,\n",
    "                     \"queries\": queries,\n",
    "                     \"num_queries\": len(queries),\n",
    "                     \"warpings\": dict(),\n",
    "                     \"distances\": dict(),\n",
    "                     'warp_dist': dict(),\n",
    "                     \"queriesID\": list(queries.keys()),\n",
    "                     \"time_distortion\": defaultdict(dict),\n",
    "                     \"distance_distortion\": defaultdict(dict),\n",
    "                     'warpings_per_step_pattern': defaultdict(dict),\n",
    "                     'feat_weights': 1.0}\n",
    "\n",
    "        self.data_open_ended = {\"ref_id\": ref_id,\n",
    "                                \"reference\": reference,\n",
    "                                \"queries\": defaultdict(list),\n",
    "                                'warp_dist': dict()}\n",
    "        scale_params = dict()\n",
    "\n",
    "        for pv_dict in self.data['reference']:\n",
    "            pv_name = pv_dict['name']\n",
    "            pv_min = min(pv_dict['values'])\n",
    "            pv_max = max(pv_dict['values'])\n",
    "            scale_params[pv_name] = (pv_min, pv_max)\n",
    "\n",
    "        self.scale_params = scale_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

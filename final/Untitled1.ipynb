{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEPAGRA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308bd84e22af4d9289ec3feb5213db54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Time loop', max=415), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a5fe3507624bd1b67042646efb67df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Parameter Loop', max=162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699a57f7bd334fb9bcef89a9858abf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df236fcfbe4744a6ad4a794bb34f4608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f1ecf87e754fd48093e8d9f9de21ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a152e0be557546bf85cc770f71d1a9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df802ee2062a4019ae92689ea67cbe83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b3857ad59d4d4d8ea3649d0bafc2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb33ec492fe40408363eedcf33fd743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba1a48f83ce43a6b09ad60aa4d5f07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67c6a13e3624493beb254ad5ae12b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='CV loop', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan 31 11:28:58 2019\n",
    "\n",
    "@author: DEPAGRA\n",
    "\"\"\"\n",
    "import libdtw as lib\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from copy import deepcopy\n",
    "\n",
    "data = lib.load_data(100)\n",
    "step_pattern = 'symmetricP2'\n",
    "\n",
    "D = lib.Dtw(data)\n",
    "\n",
    "with open('dtwObjOptWeights16AllFeats.pickle', 'rb') as f:\n",
    "    D_weights = pickle.load(f)\n",
    "D.data['feat_weights'] = D_weights\n",
    "\n",
    "try:\n",
    "    with open('data/all_sub100_%s.pickle'%step_pattern, 'rb') as f:\n",
    "        D.data_open_ended['warp_dist'] = pickle.load(f)\n",
    "except OSError as ex:\n",
    "    for _id in tqdm(D.data['queriesID']):\n",
    "        D.call_dtw(_id, step_pattern=step_pattern, n_jobs=1, open_ended=True, all_sub_seq=True)\n",
    "\n",
    "    with open('data/all_sub100_%s.pickle'%step_pattern, 'wb') as f:\n",
    "        pickle.dump(D.data_open_ended['warp_dist'], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#%%\n",
    "for online_id in D.data['queriesID']:\n",
    "    try:\n",
    "        online_raw = pd.read_csv('online_data_sets/online_%s_%s.csv'%(online_id, step_pattern), header=0, index_col=None, dtype={'query_id': str})\n",
    "    except OSError as ex:\n",
    "        data_set = list()\n",
    "        for (i, j, d) in D.data_open_ended['warp_dist'][online_id]:\n",
    "            data_point = {'DTW_distance': d,\n",
    "                          'length': j + 1,\n",
    "                          'query_id' : online_id,\n",
    "                          'true_length': len(data[online_id][0]['values']),\n",
    "                          'ref_len': len(data[data['reference']][0]['values']),\n",
    "                          'ref_prefix': i+1,\n",
    "                          'step_pattern': step_pattern}\n",
    "            data_set.append(data_point)\n",
    "        online_raw = pd.DataFrame(data_set)\n",
    "        online_raw.to_csv('online_data_sets/online_%s_%s.csv'%(online_id, step_pattern), header=True, index = False,)\n",
    "\n",
    "\n",
    "#%%\n",
    "np.random.shuffle(D.data['queriesID'])\n",
    "for online_id in D.data['queriesID']:\n",
    "    true_length = len(data[online_id][0]['values'])\n",
    "    try:\n",
    "        with open('estimates_all_sub/%s.pickle'%online_id, 'rb') as f:\n",
    "            estimates = pickle.load(f)\n",
    "    except OSError as ex:\n",
    "\n",
    "        try:\n",
    "            online_raw = pd.read_csv('online_data_sets/online_%s_%s.csv'%(online_id, step_pattern), header=0, index_col=None, dtype={'query_id': str})\n",
    "        except OSError as ex:\n",
    "\n",
    "            online_raw = D.generate_train_set(step_pattern=step_pattern, query_id=online_id, n_jobs = -1, open_ended=True, all_sub_seq=True)\n",
    "            online_raw.to_csv('online_data_sets/online_%s_%s.csv'%(online_id, step_pattern), header=True, index = False,)\n",
    "        estimates = list()\n",
    "        for online_t in online_raw['length'].values[50:]:\n",
    "            print('\\n')\n",
    "            print(online_id)\n",
    "            t_prime = online_raw.loc[online_raw['length']==online_t, 'ref_prefix'].values[0] - 1\n",
    "\n",
    "            try:\n",
    "                online_raw.index = online_raw['query_id']\n",
    "                online = online_raw.drop(columns=['query_id', 'ref_len', 'step_pattern'])\n",
    "\n",
    "            except: pass\n",
    "\n",
    "\n",
    "            data_set = list()\n",
    "\n",
    "            for _id, warp_dist in D.data_open_ended['warp_dist'].items():\n",
    "                if _id != online_id:\n",
    "                    mapped_points = list(filter(lambda x: x[0]==t_prime, warp_dist))\n",
    "                    for (i, j, d) in mapped_points:\n",
    "                        data_point = {'DTW_distance': d,\n",
    "                                      'length': j + 1,\n",
    "                                      'query_id' : _id,\n",
    "                                      'true_length': len(data[_id][0]['values'])}\n",
    "                        data_set.append(data_point)\n",
    "\n",
    "            def build_structured_array(data_set):\n",
    "                output = list()\n",
    "                for idx, row in data_set.iterrows():\n",
    "                    survival_time = row['true_length'] - row['length']\n",
    "                    output.append((True, survival_time))\n",
    "                res = np.array(output, dtype = [('status', bool), ('time_remaining', 'f8')])\n",
    "                return res\n",
    "\n",
    "            data_set = pd.DataFrame(data_set)\n",
    "            data_set.index = data_set['query_id']\n",
    "            data_y = build_structured_array(data_set)\n",
    "            data_set.drop(columns=['query_id'], inplace = True)\n",
    "\n",
    "            for _id, row in data_set.iterrows():\n",
    "                batch = D.data['queries'][_id]\n",
    "                length = int(row['length'])\n",
    "                for pv in batch:\n",
    "                    data_set.at[_id, pv['name']] = pv['values'][length - 1]\n",
    "\n",
    "        #    corr_matrix = data_set.corr().abs()\n",
    "        #    corr_matrix.fillna(1, inplace = True)\n",
    "        #    # Select upper triangle of correlation matrix\n",
    "        #    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        #\n",
    "        #    # Find index of feature columns with correlation greater than 0.95\n",
    "        #    to_drop = [column for column in upper.columns if any(upper[column] > 0.99) and column != 'true_length']\n",
    "        #\n",
    "        #    # Drop features\n",
    "        #    data_set.drop(columns = to_drop, inplace = True)\n",
    "            #print(data_set.shape, '\\n', data_set.columns)\n",
    "\n",
    "            data_x = data_set.drop(columns = ['true_length'])\n",
    "\n",
    "            model = gbsa()\n",
    "            model.fit(data_x.reindex(sorted(data_x.columns), axis=1), data_y)\n",
    "            score = model.score(data_x.reindex(sorted(data_x.columns), axis=1), data_y); print(score)\n",
    "\n",
    "            data_set_risk = data_set\n",
    "            data_set_risk['risk'] = model.predict(data_x.reindex(sorted(data_x.columns), axis=1))\n",
    "            data_set_risk.sort_values(by='risk', ascending=False, inplace=True)\n",
    "\n",
    "            lengths = data_set_risk['true_length']\n",
    "            #    fig = plt.figure()\n",
    "            #    plt.bar(np.arange(1, len(lengths)+1), lengths.values)\n",
    "            #    plt.show()\n",
    "\n",
    "\n",
    "            for _id, row in online.iterrows():\n",
    "                batch = D.data['queries'][_id]\n",
    "                length = int(row['length'])\n",
    "                for pv in batch:\n",
    "                    online.at[_id, pv['name']] = pv['values'][length - 1]\n",
    "\n",
    "            online_clean = online.loc[:, data_x.columns]\n",
    "            new_x = online_clean.loc[online['length'] == online_t, :]\n",
    "            new_x['risk'] = model.predict(new_x.reindex(sorted(data_x.columns), axis=1))\n",
    "            new_x['true_length'] = len(data[online_id][0]['values'])\n",
    "\n",
    "            data_set_new = pd.concat([data_set, new_x])\n",
    "            data_y_new = build_structured_array(data_set_new)\n",
    "            data_x_new = data_set_new.drop(columns=['true_length', 'risk'])\n",
    "\n",
    "            risk_complete = model.predict(data_x_new.reindex(sorted(data_x.columns), axis=1))\n",
    "            new_score = model.score(data_x_new.reindex(sorted(data_x.columns), axis=1), data_y_new); print(new_score)\n",
    "            data_set_new['risk'] = risk_complete\n",
    "            data_set_new['time_remaining'] = data_y_new['time_remaining']\n",
    "\n",
    "            data_set_new.sort_values(by='risk', ascending=False, inplace=True)\n",
    "\n",
    "            t_left = data_set_new['time_remaining']\n",
    "            risks = data_set_new['risk']\n",
    "            loc = data_set_new.index.get_loc(online_id)+1\n",
    "        #        plt.figure()\n",
    "        #        plt.bar(np.arange(1, len(t_left)+1), t_left.values)\n",
    "        #        plt.bar(loc, t_left[online_id], color = 'r')\n",
    "        #        plt.xticks(np.arange(1, len(t_left)+1), data_set_new.index, rotation = 90)\n",
    "        #        plt.show()\n",
    "            xy = [(x,y) for x, y in zip(np.arange(1, len(t_left)), t_left.values) if x != loc]\n",
    "            x = np.array([x[0] for x in xy]).reshape(-1,1)\n",
    "            y = np.array([x[1] for x in xy]).reshape(-1,1)\n",
    "            lm = LinearRegression().fit(x, y)\n",
    "            est = lm.predict(loc)\n",
    "            estimates.append(est[0][0]+online_t)\n",
    "            print('t: %d\\tT: %d\\test: %d\\tavg: %0.f'%(online_t, true_length, est[0][0]+online_t, pd.Series(estimates).rolling(60).mean().fillna(method='bfill').values[-1]))\n",
    "\n",
    "        with open('estimates_all_sub/%s.pickle'%online_id, 'wb') as f:\n",
    "            pickle.dump(estimates, f)\n",
    "    #fig = plt.figure()\n",
    "        plt.plot(np.arange(51, true_length+1), estimates, label = 'Estimate')\n",
    "        plt.plot(np.arange(51, true_length+1), pd.Series(estimates).rolling(60).mean().fillna(method='bfill'), color = 'r', label = '60mins MA estimate')\n",
    "        plt.plot(x=[51, true_length], y=[51, true_length], color = \"gray\")\n",
    "        plt.hlines(true_length+np.sqrt(true_length), 51, true_length, color = 'grey', label = '+- Poisson SD')\n",
    "        plt.hlines(true_length-np.sqrt(true_length), 51, true_length, color = 'grey')\n",
    "        plt.hlines(true_length, 51, 51+len(estimates), label = 'True value')\n",
    "        plt.legend()\n",
    "        plt.xlim((51, true_length))\n",
    "        plt.title(online_id)\n",
    "        plt.savefig('pics/all_sub/%s.png'%online_id)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    #pd.Series(estimates).rolling(5).mean().fillna(0)\n",
    "    \n",
    "\n",
    "#%%\n",
    "class Estimator:\n",
    "    \n",
    "    def __init__(self, dtw_obj=D, regressor=LinearRegression(), loss='coxph', learning_rate=0.1, n_estimators=100, max_depth=3, subsample=1.0, random_state=42):\n",
    "        self.regressor = regressor\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        \n",
    "        self.dtw_obj = dtw_obj\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        self.model = GradientBoostingSurvivalAnalysis(loss=self.loss,\n",
    "                                                 learning_rate = self.learning_rate,\n",
    "                                                 n_estimators=self.n_estimators,\n",
    "                                                 max_depth=self.max_depth,\n",
    "                                                 subsample=self.subsample,\n",
    "                                                 random_state = self.random_state)\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.model.fit(self.x_train, self.y_train)\n",
    "        \n",
    "        self.data_set = pd.concat([self.x_train, pd.Series(data=self.y_train['time_remaining'], index=self.x_train.index, name='time_remaining')], axis=1, sort = False)\n",
    "        self.data_set['risk'] = self.model.predict(self.x_train)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, new_x):\n",
    "        x_new = pd.DataFrame(deepcopy(new_x))\n",
    "        x_new['risk'] = self.model.predict(x_new)\n",
    "        query_id = list(x_new.index)[0]\n",
    "        x_length = len(self.dtw_obj.data['queries'][query_id][0]['values'])\n",
    "        x_new['time_remaining'] = x_length -x_new['length']\n",
    "        \n",
    "        self.data_set_extd = pd.concat([self.data_set, x_new], axis = 0, sort = False)\n",
    "        self.data_set_extd.sort_values(by='risk', ascending=False, inplace=True)\n",
    "\n",
    "        locations = self.data_set_extd.index.get_loc(query_id)\n",
    "        \n",
    "        locs = list()\n",
    "        if type(locations) == slice:\n",
    "            start, stop = locations.start, locations.stop\n",
    "            locs.extend([loc for loc in np.arange(start, stop)])\n",
    "        elif type(locations) == int or type(locations) == np.int64:\n",
    "            locs = [locations]\n",
    "        elif type(locations) == np.ndarray:\n",
    "            locs = np.arange(len(locations))[locations]\n",
    "        else:\n",
    "            print('ERROR')\n",
    "            print(type(locations))\n",
    "            locs = []\n",
    "            \n",
    "        #locs = np.array([np.arange(0, len(self.data_set_extd))[idx] for idx in [self.data_set_extd.index.get_loc(query_id)]])\n",
    "        \n",
    "        t_left = self.data_set_extd['time_remaining']\n",
    "        ests = list()\n",
    "        \n",
    "        for loc in locs:\n",
    "#            print(locs)\n",
    "#            print([x for x, y in zip(np.arange(len(t_left)), t_left.values)])\n",
    "            xy = [(x,y) for (x, y) in zip(np.arange(len(t_left)), t_left.values) if x != loc]\n",
    "            x = np.array([x[0] for x in xy]).reshape(-1,1)\n",
    "            y = np.array([x[1] for x in xy])\n",
    "            \n",
    "            reg = self.regressor.fit(X=x, y=y)\n",
    "            ests.append(reg.predict(loc)[0])\n",
    "            \n",
    "        return np.array(ests)\n",
    "    \n",
    "    def score(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        return np.mean(np.abs(y_pred - y_test['time_remaining']))\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'dtw_obj': self.dtw_obj,\n",
    "                'regressor': self.regressor,\n",
    "                'loss': self.loss,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'max_depth': self.max_depth,\n",
    "                'subsample': self.subsample}\n",
    "\n",
    "    def set_params(self, parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "#        return self\n",
    "            \n",
    "#%%\n",
    "#estimator = Estimator()\n",
    "#estimator.get_params() \n",
    "#param_grid = {'regressor': [LinearRegression], \n",
    "#              'loss': ['coxph'],\n",
    "#              'learning_rate': [0.1],\n",
    "#              'n_estimators': [100],\n",
    "#              'max_depth': [3],\n",
    "#              'subsample': [1.0]}\n",
    "#\n",
    "#estimator.set_params({'loss': 'squared'})\n",
    "#\n",
    "#clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv_splitter(data_x), fit_params={'dtw_obj': D})\n",
    "#\n",
    "#clf.fit(data_x, data_y['time_remaining'])\n",
    "#    \n",
    "#\n",
    "#a = generate_dataset_xy(5, D)\n",
    "#a[0]\n",
    "#\n",
    "#cv_splitter(a[0])\n",
    "#%%\n",
    "def cv_splitter(data_set):\n",
    "    for query_id in data_set.index.unique():\n",
    "        train_id = data_set.loc[data_set.index != query_id, :].index\n",
    "        test_id = data_set.loc[data_set.index == query_id, :].index\n",
    "        \n",
    "        train_loc = list()\n",
    "        for _id in train_id.unique():\n",
    "            locs = data_set.index.get_loc(_id)\n",
    "            if type(locs) == slice:\n",
    "                start, stop = locs.start, locs.stop\n",
    "                train_loc.extend([[loc] for loc in np.arange(start, stop)])\n",
    "            elif type(locs) == int or type(locs) == np.int64:\n",
    "                train_loc.append([locs])\n",
    "            else: print('\\n', locs, type(locs))\n",
    "       \n",
    "        if type(data_set.index.get_loc(query_id)) == slice:\n",
    "            locs = data_set.index.get_loc(query_id)\n",
    "            start, stop = locs.start, locs.stop\n",
    "            test_loc = [[loc] for loc in np.arange(start, stop)]\n",
    "        elif type(data_set.index.get_loc(query_id)) == int or type(data_set.index.get_loc(query_id)) == np.int64:\n",
    "            test_loc = [data_set.index.get_loc(query_id)]\n",
    "        else: print('ERROR 2')\n",
    "        \n",
    "        yield((train_id.unique(), test_id.unique(), train_loc, test_loc))\n",
    "        \n",
    "def build_structured_array(data_set):\n",
    "    output = list()\n",
    "    for idx, row in data_set.iterrows():\n",
    "        survival_time = row['true_length'] - row['length']\n",
    "        output.append((True, survival_time))\n",
    "    res = np.array(output, dtype = [('status', bool), ('time_remaining', 'f8')])\n",
    "    return res\n",
    "\n",
    "def generate_dataset_xy(t_ref, D):\n",
    "    data_set = list()\n",
    "\n",
    "    for _id, warp_dist in D.data_open_ended['warp_dist'].items():\n",
    "        mapped_points = list(filter(lambda x: x[0]==t_ref, warp_dist))\n",
    "        for (i, j, d) in mapped_points:\n",
    "            data_point = {'DTW_distance': d,\n",
    "                          'length': j + 1,\n",
    "                          'query_id' : _id,\n",
    "                          'true_length': len(data[_id][0]['values'])}\n",
    "            data_set.append(data_point)\n",
    "        \n",
    "    data_set = pd.DataFrame(data_set)\n",
    "    data_set.index = data_set['query_id']\n",
    "    \n",
    "    data_y = build_structured_array(data_set)\n",
    "    data_set.drop(columns = ['query_id', 'true_length'], inplace = True)\n",
    "\n",
    "    for _id, row in data_set.iterrows():\n",
    "        batch = D.data['queries'][_id]\n",
    "        length = int(row['length'])\n",
    "        for pv in batch:\n",
    "            data_set.at[_id, pv['name']] = pv['values'][length - 1]\n",
    "    \n",
    "    return (data_set, data_y)\n",
    "\n",
    "def GridSearch(estimator, dtw_obj, param_grid, n_jobs, cv_splitter=cv_splitter):\n",
    "    params_iter = ParameterGrid(param_grid)\n",
    "    ref_len = len(dtw_obj.data['reference'][0]['values'])\n",
    "    \n",
    "    score_t = list()\n",
    "    for t in tqdm_notebook(np.arange(ref_len), desc='Time loop'):\n",
    "        data_x, data_y = generate_dataset_xy(t, dtw_obj)\n",
    "        score_params = list()\n",
    "        for parameters in tqdm_notebook(params_iter, desc='Parameter Loop', leave=False):\n",
    "            estimator.set_params(parameters)\n",
    "            score_cv = list()\n",
    "            for train_id, test_id, train_loc, test_loc in tqdm_notebook(cv_splitter(data_x), desc='CV loop', leave=False):\n",
    "                x_train = data_x.loc[train_id, :]\n",
    "                #print(x_train.shape)\n",
    "                y_train = np.array(np.concatenate([data_y[idx] for idx in train_loc], axis=0), dtype = [('status', bool), ('time_remaining', 'f8')])\n",
    "                #print(y_train.shape, '\\n')\n",
    "\n",
    "                x_test = data_x.loc[test_id, :]\n",
    "                y_test_raw = [data_y[idx] for idx in test_loc]\n",
    "\n",
    "                y_test = np.concatenate(y_test_raw, axis=0) if len(y_test_raw)>1 else y_test_raw\n",
    "                y_test = np.array(y_test, dtype = [('status', bool), ('time_remaining', 'f8')])\n",
    "                \n",
    "                estimator.fit(x_train, y_train)\n",
    "                \n",
    "                score = estimator.score(x_test, y_test)\n",
    "                score_cv.append(score)\n",
    "            \n",
    "            score_params.append((parameters, t, np.mean(score_cv)))\n",
    "        \n",
    "        score_t.append(score_params)\n",
    "        \n",
    "    return score_t\n",
    "                \n",
    "                \n",
    "                \n",
    "#%%\n",
    "lr = LinearRegression()\n",
    "param_grid = {'regressor': [lr], \n",
    "              'loss': ['coxph', 'squared'],\n",
    "              'learning_rate': [0.1,  0.01, 0.2],\n",
    "              'n_estimators': [100, 50, 500],\n",
    "              'max_depth': [3, 5, 10],\n",
    "              'subsample': [1.0, 0.75, 0.5]}\n",
    "\n",
    "estimator = Estimator(dtw_obj=D)\n",
    "\n",
    "scores = GridSearch(estimator=estimator, dtw_obj=D, param_grid=param_grid, n_jobs=1, cv_splitter=cv_splitter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
